import json
from openai import OpenAI
from neo4j import GraphDatabase
from typing import Optional, Any


OPENAI_API_KEY = "sk-af0bd19b890d465ea71ca754cf2a6658"
OPENAI_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "1234abcd"


class LLMClient:
    def __init__(self):
        self.client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )

    def call_llm(self, model, messages, temperature=0, max_tokens=1500):
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        ).choices[0].message.content


class KGClient:
    def __init__(self):
        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

    def read(self, query: str, params: Optional[dict]=None) -> Optional[list[dict]]:
        with self.driver.session() as session:
            result = session.run(query, params or {})
            return [record.data() for record in result]
    
    def get_event_chain(self, system: str, event: str) -> Optional[list[dict]]:
        query = """
        MATCH (s:System {name: $system})
        MATCH (s)-[:HAS_EVENT]->(e:Event {name: $event})
        MATCH (e)-[:HAS_CAUSE]->(c:Cause)
        MATCH (c)-[:HAS_SUBCAUSE]->(sc:SubCause)
        MATCH (sc)-[:CAN_BE_REPAIRED_BY]->(r:Repair)
        MATCH (r)-[:NEED_TO_BE_VALIDATED_BY]->(v:Validation)
        RETURN e AS event, c AS cause, sc AS subcause, r AS repair, v AS validation

        UNION

        MATCH (s:System {name: $system})
        MATCH (s)-[:HAS_EVENT]->(e:Event {name: $event})
        MATCH (e)-[:HAS_CAUSE]->(c:Cause)
        MATCH (c)-[:CAN_BE_REPAIRED_BY]->(r:Repair)
        MATCH (r)-[:NEED_TO_BE_VALIDATED_BY]->(v:Validation)
        RETURN e AS event, c AS cause, NULL AS subcause, r AS repair, v AS validation
        """
        return self.read(query, {"system": system, "event": event})


class Coordinator:
    def __init__(self, event: str, model: str="qwen3-max", retry_times: int=3):
        self.llm_client = LLMClient()
        self.model = model
        self.retry_times = retry_times
        
        self.event = event

        system_prompt = "ä½ æ˜¯ä¸€ä¸ªå«æ˜Ÿè¿ç»´ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼šå¯¹æ•èŽ·çš„å«æ˜Ÿå¼‚å¸¸äº‹ä»¶ï¼ˆEventï¼‰è¿›è¡Œæ•…éšœæŽ’æŸ¥å’Œä¿®å¤ã€‚æ·±å‘¼å¸ï¼Œä¸€æ­¥ä¸€æ­¥æ¥ã€‚"
        self.history = [{"role": "system", "content": system_prompt}]  # è€ƒè™‘ç¼“å­˜ç®¡ç†

        self.prompt_template = """Event:{event}
å‚è€ƒèµ„æ–™:{knowledge}
å½“å‰çŽ¯èŠ‚:åŸºäºŽå‚è€ƒèµ„æ–™å°†è¯¥å¼‚å¸¸åˆæ­¥å½’å› åˆ°ä»¥ä¸‹åˆ†ç³»ç»Ÿä¸­çš„ä¸€ä¸ªï¼š
ç»“æž„, è½½è·, ç”µæº, çƒ­æŽ§, å§¿è½¨æŽ§åˆ¶, æµ‹æŽ§ä¸Žæ•°æ®å¤„ç†
åªè¾“å‡ºåˆ†ç³»ç»Ÿåç§°ï¼Œä¸éœ€è¦å¤šä½™è§£é‡Šå’Œæ ¼å¼"""
        self.prompt = self.prompt_template.format(event=self.event, knowledge=self._get_knowledge())

        self.sub_systems = {"ç»“æž„", "è½½è·", "ç”µæº", "çƒ­æŽ§", "å§¿è½¨æŽ§åˆ¶", "æµ‹æŽ§ä¸Žæ•°æ®å¤„ç†"}

    def _get_knowledge(self) -> str:
        # è€ƒè™‘RAG
        return """1.å«æ˜Ÿç”±å¤šä¸ªåˆ†ç³»ç»Ÿç»„æˆï¼ŒåŒ…æ‹¬ç»“æž„ã€è½½è·ã€ç”µæºã€çƒ­æŽ§ã€å§¿è½¨æŽ§åˆ¶ã€æµ‹æŽ§ä¸Žæ•°æ®å¤„ç†ã€‚
2.ç»“æž„åˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:æä¾›å«æ˜Ÿçš„ç‰©ç†æ”¯æ’‘ä¿æŠ¤å†…éƒ¨è®¾å¤‡å…å—å‘å°„æŒ¯åŠ¨ã€å¤ªç©ºçŽ¯å¢ƒçš„å½±å“ã€‚ç»„æˆ:æ¡†æž¶ã€å¤–å£³ã€æ”¯æž¶ç­‰è½»é‡åŒ–é«˜å¼ºåº¦ææ–™(å¦‚ç¢³çº¤ç»´ã€é“åˆé‡‘)
3.è½½è·åˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:æ‰§è¡Œå«æ˜Ÿçš„æ ¸å¿ƒä»»åŠ¡å¦‚é€šä¿¡ã€é¥æ„Ÿã€å¯¼èˆªã€ç§‘å­¦æŽ¢æµ‹ç­‰)ç»„æˆ:ç›¸æœºã€é›·è¾¾ã€é€šä¿¡è½¬å‘å™¨ã€ç§‘å­¦ä»ªå™¨ç­‰ã€‚
4.ç”µæºåˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:ä¸ºå…¨å«æ˜Ÿä¾›ç”µå¹¶ç®¡ç†èƒ½æºã€‚ç»„æˆ:å¤ªé˜³èƒ½ç”µæ± æ¿(ä¸»èƒ½æº)ã€è“„ç”µæ± ã€ç”µæºæŽ§åˆ¶ä¸Žåˆ†é…å•å…ƒã€‚
5.çƒ­æŽ§åˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:ç»´æŒè®¾å¤‡åœ¨é€‚å®œæ¸©åº¦èŒƒå›´(-40Â°Cè‡³+50Â°C)é˜²æ­¢è¿‡çƒ­æˆ–è¿‡å†·å¤±æ•ˆã€‚æ–¹å¼:è¢«åŠ¨:éš”çƒ­å±‚ã€çƒ­åå°„æ¶‚å±‚ã€çƒ­ç®¡ã€‚ä¸»åŠ¨:ç”µåŠ çƒ­å™¨ã€æ•£çƒ­å™¨ã€‚
6.å§¿è½¨æŽ§åˆ¶åˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:æŽ§åˆ¶å«æ˜Ÿåœ¨å¤ªç©ºä¸­çš„å§¿æ€(æŒ‡å‘æ–¹å‘)å’Œç¨³å®šæ€§ã€‚ç»„æˆ:ä¼ æ„Ÿå™¨:é™€èžºä»ªã€æ˜Ÿæ•æ„Ÿå™¨ã€å¤ªé˜³æ•æ„Ÿå™¨ã€‚æ‰§è¡Œæœºæž„:åä½œç”¨è½®ã€ç£åŠ›çŸ©å™¨æŽ¨è¿›å™¨ã€‚
7.æµ‹æŽ§ä¸Žæ•°æ®å¤„ç†åˆ†ç³»ç»Ÿï¼šåŠŸèƒ½:å¤„ç†å«æ˜Ÿå†…éƒ¨æ•°æ®ã€åè°ƒå„åˆ†ç³»ç»Ÿå·¥ä½œã€æŽ¥æ”¶åœ°é¢æŒ‡ä»¤ã€å‘åœ°é¢å‘é€å«æ˜ŸçŠ¶æ€æ•°æ®å’Œè½½è·æ•°æ®ç»„æˆ:å¤©çº¿ã€æ”¶å‘æœºã€æ•°æ®å­˜å‚¨è®¾å¤‡ä¸­å¤®è®¡ç®—ã€æ€»çº¿ã€æŽ¥å£æ¨¡å—"""

    def _check_format(self, response: str) -> bool:
        return response in self.sub_systems

    def route(self) -> str:
        self.history.append({"role": "user", "content": self.prompt})
        
        llm_response = self.llm_client.call_llm(model=self.model, messages=self.history).strip()
        retry_count = 0
        while not self._check_format(llm_response) and retry_count < self.retry_times:
            llm_response = self.llm_client.call_llm(
                model=self.model,
                messages=self.history + [
                    {"role": "assistant", "content": llm_response},
                    {"role": "user", "content": "è§£æžå¤±è´¥ï¼Œåªè¾“å‡ºåˆ†ç³»ç»Ÿåç§°ï¼Œä¸éœ€è¦å¤šä½™è§£é‡Šå’Œæ ¼å¼"}
                ]
            ).strip()
            retry_count += 1
        if retry_count == self.retry_times and not self._check_format(llm_response):
            llm_response = "ç”µæº"  # å…œåº•
        self.history.append({"role": "assistant", "content": llm_response})
        
        return llm_response


# class WorkerNode:
#     def __init__(self, name: str, parent: Optional["WorkerNode"], children: Optional[list]=None, attribute: Optional[dict]=None):
#         self.name = name
#         self.parent = parent
#         self.children = children
#         self.attribute = attribute

#     def get_name(self) -> str:
#         return self.name
    
#     def get_parent(self) -> Optional["WorkerNode"]:
#         return self.parent

#     def add_children(self, children: list):
#         if not self.children:
#             self.children = []
#         self.children.extend(children.copy())

#     def get_children(self) -> Optional[list]:
#         return self.children

#     def set_attribute(self, key: str, value: Any):
#         if not self.attribute:
#             self.attribute = {}
#         self.attribute[key] = value

#     def get_attribute(self) -> Optional[dict]:
#         return self.attribute


# class Worker:
#     def __init__(self, excel_at: str, event: str, model: str="qwen3-max", retry_times: int=3):
#         self.llm_client = LLMClient()
#         self.model = model
#         self.retry_times = retry_times

#         self.kg_client = KGClient()
        
#         # å›¾è°±ç®¡ç† (More: å¤§æ¨¡åž‹æŽ¢ç´¢èƒ½åŠ›/æŒä¹…åŒ–ç»éªŒreflection)
#         self.event = event
#         self.root = WorkerNode(name=self.event, parent=None, attribute={"type": "event"})
#         self.cursor = self.root
        
#         # å¯¹è¯ç®¡ç† (More: è®°å¿†ç®¡ç†)
#         system_prompt = f"ä½ æ˜¯ä¸€ä¸ªç²¾é€š{excel_at}çš„å«æ˜Ÿè¿ç»´ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼šå¯¹æ•èŽ·çš„å«æ˜Ÿå¼‚å¸¸äº‹ä»¶ï¼ˆEventï¼‰è¿›è¡Œæ•…éšœæŽ’æŸ¥å’Œä¿®å¤ã€‚æ·±å‘¼å¸ï¼Œä¸€æ­¥ä¸€æ­¥æ¥ã€‚"
#         self.history = [{"role": "system", "content": system_prompt}]

#         self.prompt_template = """Event:{event}
# å®žæ—¶é¥æµ‹:{telemetry_block}
# çŸ¥è¯†å›¾è°±:{kg_block}
# å…·ä½“ä»»åŠ¡:{instruction_block}
# è¦æ±‚:{format_block}"""

#     def _get_telemetry_block(self):
#         # æ¼”ç¤ºéœ€è¦ï¼Œè¿›è¡Œç®€åŒ–
#         import json
#         return json.dumps({
#             "battery_voltage_last10_mean": 26.3,
#             "array_power_last10_mean": 132.0,
#             "battery_temp_last10_min": 11,
#             "bus_current_sensor_bias": "suspected",
#         }, ensure_ascii=False, indent=2)

#     def _get_kg_block(self):
#         pass

#     def _get_instruction_block(self):
#         pass

#     def _get_format_block(self):
#         # æ¼”ç¤ºéœ€è¦ï¼Œè¿›è¡Œç®€åŒ–
#         return "åªè¾“å‡ºç¼–å·ï¼Œä¸éœ€è¦å¤šä½™è§£é‡Šå’Œæ ¼å¼"

#     def run(self):
#         # LOOP
#         pass


class Worker:
    def __init__(self, excel_at: str, system: str, event: str, model: str="qwen3-max", retry_times: int=3):
        self.llm_client = LLMClient()
        self.model = model
        self.retry_times = retry_times

        self.kg_client = KGClient()
        
        self.system = system
        self.event = event
        
        system_prompt = f"ä½ æ˜¯ä¸€ä¸ªç²¾é€š{excel_at}çš„å«æ˜Ÿè¿ç»´ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼šå¯¹æ•èŽ·çš„å«æ˜Ÿå¼‚å¸¸äº‹ä»¶ï¼ˆEventï¼‰è¿›è¡Œæ•…éšœæŽ’æŸ¥å’Œä¿®å¤ã€‚æ·±å‘¼å¸ï¼Œä¸€æ­¥ä¸€æ­¥æ¥ã€‚"
        self.history = [{"role": "system", "content": system_prompt}]

        self.prompt_template = """Event:{event}
å®žæ—¶é¥æµ‹:{telemetry_block}
çŸ¥è¯†å›¾è°±:{kg_block}
å…·ä½“ä»»åŠ¡:{instruction_block}
è¦æ±‚:{format_block}"""

    def _get_telemetry_block(self) -> str:
        return """{
  "timestamp": "2025-11-20T23:47:12Z",
  "event_detected": "æ¯çº¿ç”µåŽ‹å¼‚å¸¸",
  "system": "ç”µæºåˆ†ç³»ç»Ÿ",
  "telemetry": {
    "bus_voltage": {
      "value": 31.4,
      "unit": "V",
      "nominal": 28.0,
      "deviation_percent": 12.1,
      "status": "out_of_range"
    },
    "bus_current": {
      "value": 9.8,
      "unit": "A",
      "nominal": 8.5,
      "fluctuation_ripple": 0.42,
      "status": "unstable"
    },
    "dcdc_output_voltage": {
      "module_id": "DCDC-1A",
      "value": 29.7,
      "unit": "V",
      "nominal": 28.0,
      "drift_detected": true
    },
    "dcdc_temperature": {
      "value": 72.5,
      "unit": "C",
      "nominal": 55.0,
      "limit": 80.0,
      "status": "warning"
    },
    "pcu_status": {
      "voltage_reg_fail_count": 7,
      "last_fail_timestamp": "2025-11-20T23:45:03Z",
      "mode": "regulation_fault"
    },
    "fault_flags": {
      "bus_overvoltage": true,
      "dcdc_drift_flag": true,
      "load_surge_detected": false
    }
  }
}"""

    def _get_kg_block(self) -> str:
        # å‡è®¾åœ¨å›¾è°±ä¸­æœ‰åç§°ä¸Žself.eventç›¸åŒçš„èŠ‚ç‚¹
        kg = self.kg_client.get_event_chain(system=self.system, event=self.event)

        if not kg:
            return """æœªæ£€ç´¢åˆ°çŸ¥è¯†å›¾è°±"""
        
        paths = []
        for row in kg:
            e = row["event"]
            c = row["cause"]
            sc = row["subcause"]
            r = row["repair"]
            v = row["validation"]
            if sc:
                path = (
                    f"{e} ->HAS_CAUSE-> {c} "
                    f"->HAS_SUBCAUSE-> {sc} "
                    f"->CAN_BE_REPAIRED_BY-> {r} "
                    f"->NEED_TO_BE_VALIDATED_BY-> {v}"
                )
            else:
                path = (
                    f"{e} ->HAS_CAUSE-> {c} "
                    f"->CAN_BE_REPAIRED_BY-> {r} "
                    f"->NEED_TO_BE_VALIDATED_BY-> {v}"
                )
            paths.append(path)

        return "\n".join(paths)

    def _get_instruction_block(self) -> str:
        return """åŸºäºŽçŸ¥è¯†å›¾è°±çš„å€™é€‰è·¯å¾„ï¼Œäº§å‡ºæœ‰çº¦æŸã€å¯æ‰§è¡Œçš„ä¿®å¤å»ºè®®åˆ—è¡¨ã€‚"""

    def _get_format_block(self) -> str:
        return """1) æ ¹æ®å€™é€‰è·¯å¾„ä¸Žå¸¸è¯†ï¼Œç»™å‡ºæŽ’åºåŽçš„ä¿®å¤å»ºè®®æ¸…å•ï¼ˆtop 3 å³å¯ï¼‰ã€‚æ¯æ¡å»ºè®®éœ€åŒ…å«ï¼š
- repair_action: å…·ä½“ä¿®å¤åŠ¨ä½œï¼ˆä¸¥æ ¼å¯¹åº”ä¸Šè¿°å€™é€‰æˆ–å…¶ç­‰ä»·å·¥ç¨‹è¡¨è¿°ï¼‰
- target_nodes: æ¶‰åŠçš„åŽŸå› /å­å› å­ï¼ˆåˆ—è¡¨ï¼‰
- preconditions: æ‰§è¡Œè¯¥åŠ¨ä½œçš„å‰ç½®æ¡ä»¶æˆ–é€‚ç”¨åœºæ™¯ï¼ˆåˆ—è¡¨ï¼‰
- verification_metrics: ä¿®å¤åŽéœ€é‡ç‚¹è§‚æµ‹çš„éªŒè¯æŒ‡æ ‡ï¼ˆåˆ—è¡¨ï¼‰
- confidence: [0,1] ä¿¡å¿ƒåº¦ï¼ˆè€ƒè™‘è¯¥è·¯å¾„ä¸Žé¥æµ‹æ˜¯å¦å»åˆï¼‰
- score: [0,1] ç»¼åˆè¯„åˆ†ï¼ˆå…¼é¡¾æ”¶ç›Š/é£Žé™©/å¯å®žæ–½æ€§ï¼‰
- brief_reason: 1-2 å¥ç®€çŸ­ç†ç”±ï¼ˆä¸è¦å±•å¼€æ€ç»´é“¾ç»†èŠ‚ï¼‰

2) å…¨é‡è¿”å›ž JSONï¼Œå­—æ®µï¼š
{{
  "event": "...",
  "recommendations": [
    {{
      "repair_action": "...",
      "target_nodes": ["cause/sub-cause", "..."],
      "preconditions": ["..."],
      "verification_metrics": ["..."],
      "confidence": 0-1,
      "score": 0-1,
      "brief_reason": "..."
    }}
  ]
}}

åªè¿”å›ž JSONï¼Œä¸è¦å¤šä½™æ–‡å­—ã€‚"""

    def _get_prompt(self) -> str:
        return self.prompt_template.format(event=self.event, telemetry_block=self._get_telemetry_block(), kg_block=self._get_kg_block(), instruction_block=self._get_instruction_block(), format_block=self._get_format_block())

    def _check_format(self, response: str) -> bool:
        try:
            json.loads(response)
        except json.JSONDecodeError:
            return False
        finally:
            return True

    def run(self):
        prompt = self._get_prompt()
        self.history.append({"role": "user", "content": prompt})

        llm_response = self.llm_client.call_llm(model=self.model, messages=self.history).strip()
        retry_count = 0
        while not self._check_format(llm_response) and retry_count < self.retry_times:
            llm_response = self.llm_client.call_llm(
                model=self.model,
                messages=self.history + [
                    {"role": "assistant", "content": llm_response},
                    {"role": "user", "content": "è§£æžå¤±è´¥ï¼Œåªè¿”å›ž JSONï¼Œä¸è¦å¤šä½™æ–‡å­—ã€‚"}
                ]
            ).strip()
            retry_count += 1
        if retry_count == self.retry_times and not self._check_format(llm_response):
            raise json.JSONDecodeError
        self.history.append({"role": "assistant", "content": llm_response})
        
        return llm_response


def run(event: str):
    router = Coordinator(event)
    result = router.route()
    print("âš ï¸  event:", event)
    print("ðŸš€  route to:", result)

    energy_worker = Worker(excel_at="ç”µæºåˆ†ç³»ç»Ÿ", system="ç”µæºåˆ†ç³»ç»Ÿ", event=event)
    result = energy_worker.run()
    print("ðŸš©  result:", result)


if __name__ == "__main__":
    run("èƒ½æºç³»ç»Ÿä¾›ç”µå¼‚å¸¸")